{"tags":[{"name":"spark","permalink":"http://catlittlechen.com/tags/spark/","url":"/async/tags/spark.json","count":4},{"name":"踩坑日记","permalink":"http://catlittlechen.com/tags/踩坑日记/","url":"/async/tags/踩坑日记.json","count":8}],"categories":[{"name":"spark","permalink":"http://catlittlechen.com/categories/spark/","url":"/async/categories/spark.json","count":4}],"url":"/async/posts/2017/07/01/那些年不懂的spark-4.json","date":1498879160000,"path":{"year":2017,"month":7,"day":1,"name":"那些年不懂的spark-4"},"title":"那些年不懂的spark(4)","permalink":"http://catlittlechen.com/2017/07/01/那些年不懂的spark-4/","content":"<h2 id=\"spark-files\"><a href=\"#spark-files\" class=\"headerlink\" title=\"spark-files\"></a>spark-files</h2><p>spark-default.xml<br><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spark<span class=\"class\">.files</span><span class=\"class\">.maxPartitionBytes</span> <span class=\"number\">67108864</span></span><br><span class=\"line\">spark<span class=\"class\">.sql</span><span class=\"class\">.files</span><span class=\"class\">.maxPartitionBytes</span> <span class=\"number\">33554432</span></span><br></pre></td></tr></table></figure></p>\n<p>spark在读取大文件的时候会自动的分区，这时候分区的标准就是上面这两个参数．<br>因此可以根据这两个参数调配任务的task，防止内存爆了．</p>\n<h2 id=\"spark-dynamicAllocation\"><a href=\"#spark-dynamicAllocation\" class=\"headerlink\" title=\"spark-dynamicAllocation\"></a>spark-dynamicAllocation</h2><p>spark-default.xml<br><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spark<span class=\"class\">.dynamicAllocation</span><span class=\"class\">.enabled</span>  true</span><br><span class=\"line\">spark<span class=\"class\">.dynamicAllocation</span><span class=\"class\">.executorIdleTimeout</span> <span class=\"number\">10s</span></span><br></pre></td></tr></table></figure></p>\n<p>dynamicAllocation 是spark自动调配executor的方式．可以尽可能的使用spark集群，<br>调节executorIdleTimeout，可以在任务使用完executor之后，尽快地释放executor，使得其它任务可以尽快的使用．</p>\n<h2 id=\"u7ED3_u5C40\"><a href=\"#u7ED3_u5C40\" class=\"headerlink\" title=\"结局\"></a>结局</h2><p>未完待续</p>\n<h2 id=\"TODO\"><a href=\"#TODO\" class=\"headerlink\" title=\"TODO\"></a>TODO</h2><p>偶尔会遇到timeout的情况，这时候一般都是内存不足，一直不是很明白spark到底怎么使用内存的．也许不是java党的缘故，有点难以作手．有空再好好剖析．</p>\n<h2 id=\"Done\"><a href=\"#Done\" class=\"headerlink\" title=\"Done\"></a>Done</h2>"}