{"tags":[{"name":"踩坑日记","permalink":"https://catlittlechen.com/tags/踩坑日记/","url":"/async/tags/踩坑日记.json","count":11},{"name":"spark","permalink":"https://catlittlechen.com/tags/spark/","url":"/async/tags/spark.json","count":4}],"categories":[{"name":"spark","permalink":"https://catlittlechen.com/categories/spark/","url":"/async/categories/spark.json","count":4}],"url":"/async/posts/2017/03/30/那些年不懂的spark-2.json","date":1490873908000,"path":{"year":2017,"month":3,"day":30,"name":"那些年不懂的spark-2"},"title":"那些年不懂的spark(2)","permalink":"https://catlittlechen.com/2017/03/30/那些年不懂的spark-2/","content":"<h2 id=\"partitionBy-amp-amp-s3\"><a href=\"#partitionBy-amp-amp-s3\" class=\"headerlink\" title=\"partitionBy &amp;&amp; s3\"></a>partitionBy &amp;&amp; s3</h2><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df<span class=\"selector-class\">.write</span><span class=\"selector-class\">.partitionBy</span>(<span class=\"string\">'date'</span>, <span class=\"string\">'index1'</span>, <span class=\"string\">'index2'</span>, <span class=\"string\">'index3'</span>).parquet(<span class=\"string\">'some/path/'</span>)</span><br></pre></td></tr></table></figure>\n<p>我用spark程序在输出parquet的过程中，通过partitionBy的column来自动生成文件夹。由于spark在写入的过程中，会扫描<code>some/path/</code>的所有目录。然后由于我的目录中有以<code>date</code>这个时间维度的标准的，这造成随着时间的推移，文件夹会越来越多，扫描的数据也就越来越慢。然后由于我使用的存储是s3，而s3不是一个真正意义上的文件系统，因而扫描文件夹的过程是一个调用api的过程，就更加慢了~</p>\n<p>解决方式：可以写入hdfs等文件系统，再通过s3-dist-cp这个工具同步到s3上。</p>\n<h2 id=\"so文件\"><a href=\"#so文件\" class=\"headerlink\" title=\"so文件\"></a>so文件</h2><p>spark 可以通过sparkContext.addFiles这个api，把so文件添加到任务实例上。</p>\n<h2 id=\"结局\"><a href=\"#结局\" class=\"headerlink\" title=\"结局\"></a>结局</h2><p>未完待续</p>\n<h2 id=\"Done\"><a href=\"#Done\" class=\"headerlink\" title=\"Done\"></a>Done</h2>"}